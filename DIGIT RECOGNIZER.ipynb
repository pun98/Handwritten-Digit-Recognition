{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIGIT RECOGNIZER (MNIST dataset) :\n",
    "I have implemented this dataset using CNN in Tensorflow. I have used 2 Convolution layers and 2 MaxPooling layers. Images which are initially flattened and are of 784 pixels are then converted to 2D image of 28*28 pixels.After passing through convolution and maxpooling layers we then flattten it again and pass through a dense layer to bring some uncertainity. Further we pass it through a dropout layer which makes some units inactive and has keep_probability of 0.8 It is done for Regularisation and helps to avoid overfitting of data. Further it goes to our output layer which gives output as labels of 0-9. Using argmax we get the argument no. of max value and hence it is our predicted number.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# i have taken the mnist data input from tensorflow\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "# one_hot means that we have encoded the digit as labels from 0-9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will convert our 784 pixel input to 2_D image of 28 * 28 pixels\n",
    "input_width=28\n",
    "input_height=28\n",
    "input_channels=1\n",
    "input_pixels=784\n",
    "# I have used 2 convolution and 2 maxpooling layers in my CNN model\n",
    "#first convolution layer has 32 units and second has 64 units\n",
    "# the filter for a unit is of size k*k and stride is 1 and padding=same\n",
    "# filter for maxpooling layer is 2*2\n",
    "n_conv1=32\n",
    "n_conv2=64\n",
    "stride_conv1=1\n",
    "stride_conv2=1\n",
    "k_conv1=5\n",
    "k_conv2=5\n",
    "k_maxpool1=2\n",
    "k_maxpool2=2\n",
    "# number of units in hidden layer is 1024\n",
    "#output layer has 10 units\n",
    "n_hidden=1024\n",
    "n_out=10\n",
    "\n",
    "input_size_to_hidden=(input_width)//(k_maxpool1*k_maxpool2)*(input_height)//(k_maxpool1*k_maxpool2)*n_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and biases for each layer are generated using Tensorflow \n",
    "weights={\n",
    "    'wcl1':tf.Variable(tf.random_normal([k_conv1,k_conv1,input_channels,n_conv1])),\n",
    "    'wcl2':tf.Variable(tf.random_normal([k_conv2,k_conv2,n_conv1,n_conv2])),\n",
    "    'whl' :tf.Variable(tf.random_normal([input_size_to_hidden,n_hidden])),\n",
    "    'out' :tf.Variable(tf.random_normal([n_hidden,n_out]))\n",
    "}\n",
    "\n",
    "biases={\n",
    "    'bcl1':tf.Variable(tf.random_normal([n_conv1])),\n",
    "    'bcl2':tf.Variable(tf.random_normal([n_conv2])),\n",
    "    'bhl':tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out':tf.Variable(tf.random_normal([n_out])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the conv function adds weights and biases to our input data and does all of stride and padding\n",
    "def conv(x, weights, bias, strides):\n",
    "    out = tf.nn.conv2d(x, weights, padding=\"SAME\", strides = [1, strides, strides, 1])\n",
    "    out = tf.nn.bias_add(out, bias)\n",
    "    out = tf.nn.relu(out)\n",
    "    return out\n",
    "# maxpooling function does the pooling which is used to avoid mainly overfitting in our model \n",
    "def maxpooling(x, k):\n",
    "    return tf.nn.max_pool(x, padding = \"SAME\", ksize = [1, k, k, 1], strides = [1, k, k, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn function defines all of our convolution and maxpooling layers\n",
    "def cnn(x,weights,biases,keep_prob):\n",
    "    x=tf.reshape(x,shape=[-1,input_height,input_width,input_channels])\n",
    "    conv1=conv(x,weights['wcl1'],biases['bcl1'],stride_conv1)\n",
    "    conv1_pool=maxpooling(conv1,k_maxpool1)\n",
    "    \n",
    "    conv2=conv(conv1_pool,weights['wcl2'],biases['bcl2'],stride_conv2)\n",
    "    conv2_pool=maxpooling(conv2,k_maxpool2)\n",
    "    \n",
    "# now here we have our hidden layers that applies relu activation function\n",
    "# we also have a dropout layer that brings some uncertainity needed and has keep probability of 0.8 \n",
    "    hidden_input=tf.reshape(conv2_pool,shape=[-1,input_size_to_hidden])\n",
    "    hidden_output_before_activation=tf.add(tf.matmul(hidden_input,weights['whl']),biases['bhl'])\n",
    "    hidden_output_before_dropout=tf.nn.relu(hidden_output_before_activation)\n",
    "    hidden_output=tf.nn.dropout(hidden_output_before_dropout,keep_prob)\n",
    "    \n",
    "    output=tf.add(tf.matmul(hidden_output,weights['out']),biases['out'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, input_pixels])\n",
    "y = tf.placeholder(tf.int32, [None, n_out])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "pred = cnn(x, weights, biases, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred,labels=y))\n",
    "#this function measures the softmax cross entropy between the predictions and actual label y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "optimize = optimizer.minimize(cost)\n",
    "#this optimizer runs the Adam algorithm and minimizes the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2784669.76663208\n",
      "397418.98651123047\n",
      "210723.9832226038\n",
      "130809.74623095989\n",
      "86256.00763153154\n",
      "63309.639527991414\n",
      "47508.198832422495\n",
      "37339.08620789318\n",
      "28907.797228210286\n",
      "24601.977367603882\n",
      "18381.862570186408\n",
      "17769.399864750394\n",
      "15595.486658436887\n",
      "12152.905498364093\n",
      "10192.026421776842\n",
      "9810.250925540902\n",
      "8378.169571807022\n",
      "7542.190902253389\n",
      "7251.90639526397\n",
      "5277.771161342505\n",
      "6823.323988132179\n",
      "5242.879463316202\n",
      "5584.342637205851\n",
      "5281.43017841205\n",
      "4233.545157329124\n"
     ]
    }
   ],
   "source": [
    "# By providing all data at one go we can lead to overfitting of data \n",
    "#so to avoid it we pass input images in batches rather than all at once\n",
    "# for every batch it calculates cost and we see that cost keeps decreasing(optimizing)\n",
    "#mnist.train.next_batch itself provides the batch of given size \n",
    "batch_size = 100\n",
    "for i in range(25):\n",
    "    num_batches = int(mnist.train.num_examples/batch_size)\n",
    "    total_cost = 0\n",
    "    for j in range(num_batches):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        c, _ = sess.run([cost,optimize], feed_dict={x:batch_x , y:batch_y, keep_prob:0.8})\n",
    "        total_cost += c\n",
    "    print(total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9841"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we then get predictions in binary format for 10 classes\n",
    "# by getting argument of max value we get our prediction\n",
    "predictions=tf.argmax(pred,1)\n",
    "correct_labels=tf.argmax(y,1)\n",
    "correct_predictions=tf.equal(predictions,correct_labels)\n",
    "predictions,correct_pred=sess.run([predictions,correct_predictions],feed_dict={x:mnist.test.images, y:mnist.test.labels, keep_prob:1.0})\n",
    "correct_pred.sum()\n",
    "#tf.equal gives all those predictions which are equal to our correct_labels\n",
    "#out of 10000 images we have predicted 9841 correct (98.41 % accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
